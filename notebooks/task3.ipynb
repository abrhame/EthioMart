import pandas as pd
import torch
from torch.utils.data import Dataset, random_split
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments

# Load your labeled data
def load_conll_file(file_path):
    sentences = []
    labels = []
    current_sentence = []
    current_labels = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:  # If not a blank line
                token, label = line.split()
                current_sentence.append(token)
                current_labels.append(label)
            else:  # Blank line indicates the end of a sentence
                sentences.append(current_sentence)
                labels.append(current_labels)
                current_sentence = []
                current_labels = []

    return sentences, labels

# Load the dataset
file_path = 'labeled_telegram_product_price_location.txt'  # Replace with your file path
sentences, labels = load_conll_file(file_path)

# Load the tokenizer for XLM-Roberta
model_name = 'xlm-roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Create a label mapping
unique_labels = set(label for sublist in labels for label in sublist)
label2id = {label: i for i, label in enumerate(unique_labels)}  # Mapping from label to ID
id2label = {i: label for label, i in label2id.items()}  # Mapping from ID to label

# Function to tokenize and align labels
def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, is_split_into_words=True, return_tensors="pt")
    
    # Align labels
    aligned_labels = []
    for i, label in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get word IDs for each token
        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # Initialize with -100
        for j, label_id in enumerate(label):
            if word_ids[j] is not None:  # Ignore padding
                label_ids[word_ids[j]] = label2id[label_id]  # Use the corresponding ID
        aligned_labels.append(label_ids)

    return tokenized_inputs, aligned_labels

# Tokenize and align labels
tokenized_inputs, aligned_labels = tokenize_and_align_labels(sentences, labels)

# Create a dataset class for NER
class NERDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = [torch.tensor(label) for label in labels]  # Ensure labels are tensors

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

    def __len__(self):
        return len(self.labels)

# Create the dataset
dataset = NERDataset(tokenized_inputs, aligned_labels)

# Split the dataset into training and validation sets
train_size = int(0.8 * len(dataset))  # 80% for training
val_size = len(dataset) - train_size   # 20% for validation
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',  # Use 'eval_strategy' instead of 'evaluation_strategy'
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Load the pre-trained model for token classification
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label2id), id2label=id2label)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # Pass the evaluation dataset here
)

# Fine-tune the model
trainer.train()
